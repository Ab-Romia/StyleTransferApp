{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# \ud83c\udfa8 Advanced Neural Style Transfer Training\n",
    "## Training AdaIN, CNN, and ViT Models on Google Colab Free GPU\n",
    "\n",
    "This notebook trains three state-of-the-art style transfer models:\n",
    "- **AdaIN**: Fast arbitrary style transfer (100x faster)\n",
    "- **CNN**: VGG-based traditional approach\n",
    "- **ViT**: Vision Transformer with global context\n",
    "\n",
    "### Dataset\n",
    "- **Content**: MS-COCO 2017 (naturalistic images)\n",
    "- **Style**: WikiArt (diverse artistic styles)\n",
    "\n",
    "### Hardware Requirements\n",
    "- Google Colab Free GPU (T4 with ~15GB VRAM)\n",
    "- Training time: ~3-4 hours per model\n",
    "\n",
    "---\n",
    "\n",
    "- \u2705 Added memory cleanup between models\n",
    "- \u2705 Added error handling for checkpoints\n",
    "- \u2705 Added dataset validation\n",
    "- \u2705 Optimized for Colab Free GPU constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udce6 Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "!git clone https://github.com/Ab-Romia/StyleTransferApp.git\n",
    "%cd StyleTransferApp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install -q timm lpips matplotlib seaborn tqdm Pillow tensorboard kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udce5 Download Datasets\n",
    "\n",
    "We'll use:\n",
    "- **WikiArt**: 80K+ artistic images across 27 styles\n",
    "- **MS-COCO 2017 Train**: 118K naturalistic images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "# Create data directories\n",
    "data_dir = Path('data')\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "content_dir = data_dir / 'coco'\n",
    "style_dir = data_dir / 'wikiart'\n",
    "\n",
    "content_dir.mkdir(exist_ok=True)\n",
    "style_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download MS-COCO 2017 Train (subset for faster training on Colab Free)\n",
    "# We'll download and use 10K images to fit in Colab's disk space\n",
    "\n",
    "print(\"Downloading MS-COCO dataset...\")\n",
    "!wget -q http://images.cocodataset.org/zips/train2017.zip -P data/\n",
    "print(\"Extracting...\")\n",
    "!unzip -q data/train2017.zip -d data/\n",
    "!rm data/train2017.zip\n",
    "\n",
    "# Move images properly\n",
    "if Path('data/train2017').exists():\n",
    "    for img in Path('data/train2017').glob('*'):\n",
    "        shutil.move(str(img), str(content_dir / img.name))\n",
    "    shutil.rmtree('data/train2017')\n",
    "\n",
    "# Keep only 10K images for Colab (saves disk space)\n",
    "import random\n",
    "coco_images = list(content_dir.glob('*.jpg'))\n",
    "if len(coco_images) > 10000:\n",
    "    images_to_remove = random.sample(coco_images, len(coco_images) - 10000)\n",
    "    for img in images_to_remove:\n",
    "        img.unlink()\n",
    "    print(f\"Kept 10,000 images for training\")\n",
    "else:\n",
    "    print(f\"Using all {len(coco_images)} images\")\n",
    "\n",
    "# Validate we have enough data\n",
    "if len(list(content_dir.glob('*.jpg'))) < 1000:\n",
    "    raise RuntimeError(f\"Insufficient content images. Need at least 1000, got {len(list(content_dir.glob('*.jpg')))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download WikiArt dataset\n",
    "# For best results, use Kaggle dataset. Otherwise, we'll use a curated collection.\n",
    "\n",
    "use_kaggle = True  # Set to False to skip Kaggle and use alternative\n",
    "\n",
    "if use_kaggle:\n",
    "    try:\n",
    "        print(\"Downloading WikiArt dataset from Kaggle...\")\n",
    "        print(\"Note: You need to upload your kaggle.json to authenticate\")\n",
    "        print(\"Get it from: https://www.kaggle.com/settings/account -> Create New API Token\")\n",
    "        \n",
    "        # Upload kaggle.json\n",
    "        from google.colab import files\n",
    "        print(\"\\nPlease upload your kaggle.json file:\")\n",
    "        uploaded = files.upload()\n",
    "        \n",
    "        # Setup Kaggle\n",
    "        !mkdir -p ~/.kaggle\n",
    "        !cp kaggle.json ~/.kaggle/\n",
    "        !chmod 600 ~/.kaggle/kaggle.json\n",
    "        \n",
    "        # Download WikiArt dataset  \n",
    "        !kaggle datasets download -d ipythonx/wikiart-gangogh-creating-art-gan -p data/\n",
    "        !unzip -q data/wikiart-gangogh-creating-art-gan.zip -d data/wikiart/\n",
    "        !rm data/wikiart-gangogh-creating-art-gan.zip\n",
    "    except Exception as e:\n",
    "        print(f\"Kaggle download failed: {e}\")\n",
    "        use_kaggle = False\n",
    "\n",
    "# Fallback: Download curated collection\n",
    "if not use_kaggle or len(list(style_dir.glob('**/*.jpg'))) < 50:\n",
    "    print(\"\\nDownloading curated style collection...\")\n",
    "    \n",
    "    # Download from Hugging Face or other sources\n",
    "    !pip install -q gdown\n",
    "    \n",
    "    # Alternative: Use best-of-wikiart dataset (smaller but high quality)\n",
    "    style_urls = [\n",
    "        # Add direct download URLs for famous artworks\n",
    "        (\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg/1280px-Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg\", \"starry_night.jpg\"),\n",
    "        (\"https://upload.wikimedia.org/wikipedia/commons/thumb/c/c5/Edvard_Munch%2C_1893%2C_The_Scream%2C_oil%2C_tempera_and_pastel_on_cardboard%2C_91_x_73_cm%2C_National_Gallery_of_Norway.jpg/800px-Edvard_Munch%2C_1893%2C_The_Scream%2C_oil%2C_tempera_and_pastel_on_cardboard%2C_91_x_73_cm%2C_National_Gallery_of_Norway.jpg\", \"scream.jpg\"),\n",
    "        (\"https://upload.wikimedia.org/wikipedia/commons/thumb/5/5c/Claude_Monet_-_Water_Lilies_-_1916_-_Google_Art_Project.jpg/1280px-Claude_Monet_-_Water_Lilies_-_1916_-_Google_Art_Project.jpg\", \"water_lilies.jpg\"),\n",
    "    ]\n",
    "    \n",
    "    for url, name in style_urls:\n",
    "        !wget -q {url} -O data/wikiart/{name}\n",
    "    \n",
    "    print(f\"Downloaded {len(style_urls)} style images\")\n",
    "\n",
    "# Validate style dataset\n",
    "style_count = len(list(style_dir.glob('**/*.jpg'))) + len(list(style_dir.glob('**/*.png')))\n",
    "print(f\"\\n\u2713 Content images: {len(list(content_dir.glob('*.jpg')))}\")\n",
    "print(f\"\u2713 Style images: {style_count}\")\n",
    "\n",
    "if style_count < 10:\n",
    "    print(\"\\n\u26a0\ufe0f WARNING: Very few style images. Training quality may be limited.\")\n",
    "    print(\"   Consider uploading more style images to data/wikiart/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udd0d Data Exploration and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def show_images(image_paths, titles=None, n_cols=5, figsize=(15, 3)):\n",
    "    \"\"\"Display a grid of images\"\"\"\n",
    "    n_images = len(image_paths)\n",
    "    n_rows = (n_images + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n",
    "    axes = axes.flatten() if n_images > 1 else [axes]\n",
    "    \n",
    "    for idx, img_path in enumerate(image_paths):\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        axes[idx].imshow(img)\n",
    "        axes[idx].axis('off')\n",
    "        if titles:\n",
    "            axes[idx].set_title(titles[idx], fontsize=10)\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for idx in range(n_images, len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Show sample content images\n",
    "content_images = list(Path('data/coco').glob('*.jpg'))\n",
    "sample_content = random.sample(content_images, min(10, len(content_images)))\n",
    "print(\"\ud83d\udcf7 Sample Content Images (MS-COCO):\")\n",
    "show_images(sample_content, n_cols=5, figsize=(15, 6))\n",
    "\n",
    "# Show sample style images\n",
    "style_images = list(Path('data/wikiart').glob('**/*.jpg')) + list(Path('data/wikiart').glob('**/*.png'))\n",
    "sample_styles = random.sample(style_images, min(10, len(style_images)))\n",
    "print(\"\\n\ud83c\udfa8 Sample Style Images (WikiArt):\")\n",
    "show_images(sample_styles, n_cols=5, figsize=(15, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udfaf Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration optimized for Colab Free GPU\n",
    "CONFIG = {\n",
    "    # Data\n",
    "    'content_dir': 'data/coco',\n",
    "    'style_dir': 'data/wikiart',\n",
    "    'image_size': 256,  # Reduced for Colab Free memory\n",
    "    'batch_size': 4,    # Conservative batch size for 15GB GPU\n",
    "    'num_workers': 2,\n",
    "    \n",
    "    # Training\n",
    "    'num_epochs': 20,   # Reduced for Colab session limits\n",
    "    'learning_rate': 1e-4,\n",
    "    'weight_decay': 1e-5,\n",
    "    'lr_patience': 3,\n",
    "    \n",
    "    # Model\n",
    "    'use_attention': True,\n",
    "    \n",
    "    # Loss weights (optimized for quality)\n",
    "    'content_weight': 1.0,\n",
    "    'style_weight': 100.0,\n",
    "    'perceptual_weight': 0.5,\n",
    "    'lpips_weight': 0.3,  # Reduced to save memory\n",
    "    'tv_weight': 1e-4,\n",
    "    'use_lpips': True,\n",
    "    'use_multiscale': False,  # Disabled to save memory on Colab Free\n",
    "    \n",
    "    # Optimization\n",
    "    'use_amp': True,  # Mixed precision for faster training\n",
    "    'use_tensorboard': True,\n",
    "    \n",
    "    # Logging\n",
    "    'log_interval': 50,\n",
    "    'sample_interval': 200,\n",
    "    'save_interval': 5,\n",
    "    'keep_checkpoints': 3,\n",
    "    \n",
    "    # Validation split\n",
    "    'val_split': 0.02,  # 2% for validation (saves time)\n",
    "}\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(\"=\" * 50)\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"{key:.<30} {value}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcca Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "class StyleTransferDataset(Dataset):\n",
    "    \"\"\"Optimized dataset for style transfer training\"\"\"\n",
    "    \n",
    "    def __init__(self, content_dir, style_dir, image_size=256, mode='train'):\n",
    "        self.content_dir = Path(content_dir)\n",
    "        self.style_dir = Path(style_dir)\n",
    "        self.image_size = image_size\n",
    "        self.mode = mode\n",
    "        \n",
    "        # Collect image paths\n",
    "        self.content_images = self._collect_images(self.content_dir)\n",
    "        self.style_images = self._collect_images(self.style_dir)\n",
    "        \n",
    "        if len(self.content_images) == 0:\n",
    "            raise RuntimeError(f\"No content images found in {content_dir}\")\n",
    "        if len(self.style_images) == 0:\n",
    "            raise RuntimeError(f\"No style images found in {style_dir}\")\n",
    "        \n",
    "        print(f\"Loaded {len(self.content_images)} content images\")\n",
    "        print(f\"Loaded {len(self.style_images)} style images\")\n",
    "        \n",
    "        # Transforms\n",
    "        if mode == 'train':\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.RandomResizedCrop(image_size, scale=(0.8, 1.0)),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "                transforms.ToTensor(),\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize(image_size),\n",
    "                transforms.CenterCrop(image_size),\n",
    "                transforms.ToTensor(),\n",
    "            ])\n",
    "    \n",
    "    def _collect_images(self, directory):\n",
    "        valid_extensions = {'.jpg', '.jpeg', '.png', '.bmp'}\n",
    "        images = []\n",
    "        for ext in valid_extensions:\n",
    "            images.extend(list(directory.rglob(f'*{ext}')))\n",
    "            images.extend(list(directory.rglob(f'*{ext.upper()}')))\n",
    "        return sorted(images)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.content_images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load content\n",
    "        content_path = self.content_images[idx]\n",
    "        content_img = Image.open(content_path).convert('RGB')\n",
    "        \n",
    "        # Random style\n",
    "        style_idx = np.random.randint(0, len(self.style_images))\n",
    "        style_path = self.style_images[style_idx]\n",
    "        style_img = Image.open(style_path).convert('RGB')\n",
    "        \n",
    "        # Transform\n",
    "        content_tensor = self.transform(content_img)\n",
    "        style_tensor = self.transform(style_img)\n",
    "        \n",
    "        # Random alpha for training\n",
    "        alpha = np.random.uniform(0.5, 1.0) if self.mode == 'train' else 1.0\n",
    "        \n",
    "        return {\n",
    "            'content': content_tensor,\n",
    "            'style': style_tensor,\n",
    "            'alpha': torch.tensor(alpha, dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "# Create datasets\n",
    "print(\"Creating datasets...\")\n",
    "full_dataset = StyleTransferDataset(\n",
    "    CONFIG['content_dir'],\n",
    "    CONFIG['style_dir'],\n",
    "    CONFIG['image_size'],\n",
    "    mode='train'\n",
    ")\n",
    "\n",
    "# Split into train/val\n",
    "val_size = int(len(full_dataset) * CONFIG['val_split'])\n",
    "train_size = len(full_dataset) - val_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "print(f\"\\nTrain: {len(train_dataset)} images\")\n",
    "print(f\"Val: {len(val_dataset)} images\")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=CONFIG['num_workers'],\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=CONFIG['num_workers'],\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"\\n\u2713 Train batches: {len(train_loader)}\")\n",
    "print(f\"\u2713 Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\ude80 Model Training Functions (FIXED VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import gc\n",
    "\n",
    "# Import models\n",
    "from models.adain_model import AdaINStyleTransfer\n",
    "from models.cnn_model import StyleTransferModel as CNNModel, VGGFeatures\n",
    "from models.vit_model import StyleTransferModel as ViTModel\n",
    "from models.losses import CombinedLoss\n",
    "\n",
    "class ModelTrainer:\n",
    "    \"\"\"Unified trainer for all models (FIXED VERSION)\"\"\"\n",
    "    \n",
    "    def __init__(self, model, model_name, config, device):\n",
    "        self.model = model.to(device)\n",
    "        self.model_name = model_name\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        \n",
    "        # Create output directories\n",
    "        self.output_dir = Path(f'outputs/{model_name}')\n",
    "        self.checkpoint_dir = self.output_dir / 'checkpoints'\n",
    "        self.sample_dir = self.output_dir / 'samples'\n",
    "        \n",
    "        for d in [self.checkpoint_dir, self.sample_dir]:\n",
    "            d.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Loss function setup\n",
    "        if model_name == 'AdaIN':\n",
    "            self.criterion = CombinedLoss(\n",
    "                content_weight=config['content_weight'],\n",
    "                style_weight=config['style_weight'],\n",
    "                perceptual_weight=config['perceptual_weight'],\n",
    "                lpips_weight=config['lpips_weight'],\n",
    "                tv_weight=config['tv_weight'],\n",
    "                use_lpips=config['use_lpips']\n",
    "            ).to(device)\n",
    "            self.vgg_loss = None\n",
    "        else:\n",
    "            # Initialize VGG features for loss computation\n",
    "            from models.cnn_model import content_loss, style_loss\n",
    "            self.content_loss_fn = content_loss\n",
    "            self.style_loss_fn = style_loss\n",
    "            self.vgg_loss = VGGFeatures()\n",
    "            # Remove auto device placement from VGGFeatures\n",
    "            self.vgg_loss = self.vgg_loss.to(device)\n",
    "            self.vgg_loss.eval()\n",
    "            print(f\"\u2713 VGGFeatures initialized for {model_name}\")\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, model.parameters()),\n",
    "            lr=config['learning_rate'],\n",
    "            weight_decay=config['weight_decay']\n",
    "        )\n",
    "        \n",
    "        # Scheduler\n",
    "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer,\n",
    "            mode='min',\n",
    "            factor=0.5,\n",
    "            patience=config['lr_patience'],\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        # Mixed precision\n",
    "        self.use_amp = config['use_amp']\n",
    "        if self.use_amp:\n",
    "            self.scaler = GradScaler()\n",
    "        \n",
    "        # Training state\n",
    "        self.history = defaultdict(list)\n",
    "        self.best_val_loss = float('inf')\n",
    "    \n",
    "    def train_epoch(self, train_loader, epoch):\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        self.model.train()\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{self.config['num_epochs']}\")\n",
    "        \n",
    "        for batch_idx, batch in enumerate(pbar):\n",
    "            content = batch['content'].to(self.device)\n",
    "            style = batch['style'].to(self.device)\n",
    "            alpha = batch['alpha'].to(self.device)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            if self.use_amp:\n",
    "                with autocast():\n",
    "                    loss = self._compute_loss(content, style, alpha)\n",
    "                \n",
    "                self.scaler.scale(loss).backward()\n",
    "                self.scaler.step(self.optimizer)\n",
    "                self.scaler.update()\n",
    "            else:\n",
    "                loss = self._compute_loss(content, style, alpha)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=5.0)\n",
    "                self.optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "            \n",
    "            # Save samples periodically\n",
    "            if batch_idx % (self.config['sample_interval'] // self.config['batch_size']) == 0:\n",
    "                self._save_samples(content[:2], style[:2], epoch, batch_idx)\n",
    "        \n",
    "        return epoch_loss / len(train_loader)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def validate(self, val_loader):\n",
    "        \"\"\"Validate the model\"\"\"\n",
    "        self.model.eval()\n",
    "        val_loss = 0\n",
    "        \n",
    "        for batch in tqdm(val_loader, desc=\"Validation\", leave=False):\n",
    "            content = batch['content'].to(self.device)\n",
    "            style = batch['style'].to(self.device)\n",
    "            alpha = batch['alpha'].to(self.device)\n",
    "            \n",
    "            loss = self._compute_loss(content, style, alpha)\n",
    "            val_loss += loss.item()\n",
    "        \n",
    "        return val_loss / len(val_loader)\n",
    "    \n",
    "    def _compute_loss(self, content, style, alpha):\n",
    "        \"\"\"Compute loss based on model type (FIXED VERSION)\"\"\"\n",
    "        if self.model_name == 'AdaIN':\n",
    "            output = self.model(content, style, alpha=alpha.mean().item())\n",
    "            loss, _ = self.criterion(output, content, style, return_components=True)\n",
    "        else:\n",
    "            # Format style threshold for batch processing\n",
    "            batch_size = content.size(0)\n",
    "            style_threshold = alpha.mean().view(1).expand(batch_size)\n",
    "            \n",
    "            output = self.model(content, style, style_threshold)\n",
    "            \n",
    "            # Use VGG features for loss computation\n",
    "            output_content, output_styles = self.vgg_loss(output)\n",
    "            target_content, target_styles = self.vgg_loss(content)\n",
    "            _, style_features = self.vgg_loss(style)\n",
    "            \n",
    "            c_loss = self.content_loss_fn(output_content, target_content)\n",
    "            s_loss = self.style_loss_fn(output_styles, style_features)\n",
    "            \n",
    "            loss = c_loss + self.config['style_weight'] * s_loss\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def _save_samples(self, content, style, epoch, batch_idx):\n",
    "        \"\"\"Save sample outputs\"\"\"\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            if self.model_name == 'AdaIN':\n",
    "                output = self.model(content, style, alpha=1.0)\n",
    "            else:\n",
    "                # Format style threshold\n",
    "                batch_size = content.size(0)\n",
    "                style_threshold = torch.tensor([0.8] * batch_size).to(self.device)\n",
    "                output = self.model(content, style, style_threshold)\n",
    "        \n",
    "        # Create comparison grid\n",
    "        import torchvision.utils as vutils\n",
    "        samples = torch.cat([content, style, output.clamp(0, 1)], dim=0)\n",
    "        grid = vutils.make_grid(samples, nrow=len(content), padding=2)\n",
    "        \n",
    "        save_path = self.sample_dir / f'epoch{epoch:03d}_batch{batch_idx:04d}.png'\n",
    "        vutils.save_image(grid, save_path)\n",
    "        self.model.train()\n",
    "    \n",
    "    def train(self, train_loader, val_loader):\n",
    "        \"\"\"Main training loop\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Training {self.model_name} Model\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for epoch in range(self.config['num_epochs']):\n",
    "            # Train\n",
    "            train_loss = self.train_epoch(train_loader, epoch)\n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            \n",
    "            # Validate\n",
    "            val_loss = self.validate(val_loader)\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            \n",
    "            # Learning rate scheduling\n",
    "            self.scheduler.step(val_loss)\n",
    "            \n",
    "            # Logging\n",
    "            print(f\"\\nEpoch {epoch+1}/{self.config['num_epochs']}\")\n",
    "            print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "            print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "            print(f\"  LR: {self.optimizer.param_groups[0]['lr']:.2e}\")\n",
    "            \n",
    "            # Save checkpoint\n",
    "            is_best = val_loss < self.best_val_loss\n",
    "            if is_best:\n",
    "                self.best_val_loss = val_loss\n",
    "                self._save_checkpoint(epoch, is_best=True)\n",
    "                print(f\"  \u2713 Best model saved! (val_loss: {val_loss:.4f})\")\n",
    "            \n",
    "            if (epoch + 1) % self.config['save_interval'] == 0:\n",
    "                self._save_checkpoint(epoch, is_best=False)\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"\\n\u2713 Training completed in {elapsed/3600:.2f} hours\")\n",
    "        print(f\"\u2713 Best validation loss: {self.best_val_loss:.4f}\")\n",
    "        \n",
    "        return self.history\n",
    "    \n",
    "    def _save_checkpoint(self, epoch, is_best=False):\n",
    "        \"\"\"Save model checkpoint\"\"\"\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'best_val_loss': self.best_val_loss,\n",
    "            'history': dict(self.history)\n",
    "        }\n",
    "        \n",
    "        if is_best:\n",
    "            path = self.checkpoint_dir / 'best_model.pth'\n",
    "        else:\n",
    "            path = self.checkpoint_dir / f'checkpoint_epoch{epoch:03d}.pth'\n",
    "        \n",
    "        torch.save(checkpoint, path)\n",
    "\n",
    "print(\"\u2713 Trainer class loaded (FIXED VERSION)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udfaf Train Model 1: AdaIN (Recommended)\n",
    "\n",
    "AdaIN is the fastest and most versatile model. Perfect for real-time applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize AdaIN model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Training on: {device}\")\n",
    "\n",
    "adain_model = AdaINStyleTransfer(use_attention=CONFIG['use_attention'])\n",
    "adain_trainer = ModelTrainer(adain_model, 'AdaIN', CONFIG, device)\n",
    "\n",
    "# Train\n",
    "adain_history = adain_trainer.train(train_loader, val_loader)\n",
    "\n",
    "# Save final model\n",
    "torch.save(adain_model.state_dict(), 'outputs/AdaIN/adain_final.pth')\n",
    "print(\"\\n\u2713 AdaIN model training complete!\")\n",
    "\n",
    "# Clean up GPU memory\n",
    "del adain_model, adain_trainer\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "print(\"\u2713 Memory cleaned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udfaf Train Model 2: CNN\n",
    "\n",
    "Traditional VGG-based approach with style intensity control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CNN model\n",
    "cnn_model = CNNModel()\n",
    "cnn_trainer = ModelTrainer(cnn_model, 'CNN', CONFIG, device)\n",
    "\n",
    "# Train\n",
    "cnn_history = cnn_trainer.train(train_loader, val_loader)\n",
    "\n",
    "# Save final model\n",
    "torch.save(cnn_model.state_dict(), 'outputs/CNN/cnn_final.pth')\n",
    "print(\"\\n\u2713 CNN model training complete!\")\n",
    "\n",
    "# Clean up GPU memory\n",
    "del cnn_model, cnn_trainer\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "print(\"\u2713 Memory cleaned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udfaf Train Model 3: ViT (Vision Transformer)\n",
    "\n",
    "Most advanced model with global context understanding. Best quality but slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ViT model\n",
    "vit_model = ViTModel()\n",
    "vit_trainer = ModelTrainer(vit_model, 'ViT', CONFIG, device)\n",
    "\n",
    "# Train\n",
    "vit_history = vit_trainer.train(train_loader, val_loader)\n",
    "\n",
    "# Save final model\n",
    "torch.save(vit_model.state_dict(), 'outputs/ViT/vit_final.pth')\n",
    "print(\"\\n\u2713 ViT model training complete!\")\n",
    "\n",
    "# Clean up\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcca Load Models for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpoints safely\n",
    "\n",
    "def load_model_safely(model, checkpoint_path):\n",
    "    \"\"\"Load model with error handling\"\"\"\n",
    "    if not Path(checkpoint_path).exists():\n",
    "        print(f\"\u26a0\ufe0f Warning: {checkpoint_path} not found, using current model state\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f\"\u2713 Loaded checkpoint from {checkpoint_path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"\u26a0\ufe0f Error loading checkpoint: {e}\")\n",
    "        return False\n",
    "\n",
    "# Reload models for evaluation\n",
    "adain_model = AdaINStyleTransfer(use_attention=CONFIG['use_attention']).to(device)\n",
    "cnn_model = CNNModel().to(device)\n",
    "vit_model = ViTModel().to(device)\n",
    "\n",
    "load_model_safely(adain_model, 'outputs/AdaIN/checkpoints/best_model.pth')\n",
    "load_model_safely(cnn_model, 'outputs/CNN/checkpoints/best_model.pth')\n",
    "load_model_safely(vit_model, 'outputs/ViT/checkpoints/best_model.pth')\n",
    "\n",
    "adain_model.eval()\n",
    "cnn_model.eval()\n",
    "vit_model.eval()\n",
    "\n",
    "models_dict = {\n",
    "    'AdaIN': adain_model,\n",
    "    'CNN': cnn_model,\n",
    "    'ViT': vit_model\n",
    "}\n",
    "\n",
    "print(\"\\n\u2713 All models loaded for evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcca Performance Visualizations\n",
    "\n",
    "*(Continue with the existing visualization cells from the original notebook)*"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}